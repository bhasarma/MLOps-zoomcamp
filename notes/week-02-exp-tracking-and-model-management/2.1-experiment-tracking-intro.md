# Experiment tracking

In this module of MLOps zoomcamp, we'll talk about **Experiment tracking**. 

In this video, we'll see what experiment tracking is and we'll see a tool called **MLflow**. This tool is currently used also at OLX. We'll see:
- how to install the tool and,
- how to use it. 

Let's first be clear about few important terms and concepts that we'll be using throughout this module:
- **ML experiment**: Here we are not talking about A/B testing. We are talking about planned experiment. You can think of it as the process of building an ML model.
the process of building an ML model- training, parameter tuning etc.
- **Experiment run**: It means *each trial in an ML experiemnt*. ML experiment is actually the whole process, e.g. a data scientist may start playing with some models, hyper parameters. Each of these trials is an experiment run. One experiment is basically a bunch of ML runs.
- **Run artifact**: It means any file that is associated with an ML run
- **Experiment metadata**: It is all the information that is released to the experiment. We'll see some examples later.

## What is Experiment tracking
It is the proces of keeping track of all the **relevant information** from an **ML experiment**, which includes: 
* Source code
* Environment
* Data
* Model
* Hyperparameters
* Metrics

The word **relevant information** is used, because it will depend on what experiement you are running. Sometimes for you hyperparameters may be relevant and sometimes data will be relevant, if you are running your experiment on different datasets. 

## Why is Experiment tracking important

In general because of 3 main reasons:
* Reproducibility
* Organization
* Optimization. Hyperparameter optimization tools that you may have used already for training your ML model are not still fully automated. They need some manual work.  

Most basic experiment tracking we can think of is e.g. writing in a piece of paper or in an excel sheet. Some probelems with these approches are:
- they are error prone
- No standard format
- Visibility and collaboration

## MLflow
Definition: An open source platform foe the machine learning life cycle. 

By ML lifecycle, they mean the whole process building, maintaining ML model. We'll focus on only a subset of the process. 

Not to confuse with Kubeflow. 

In practice, MLflow just a simple python package, that can be installed with `pip install`. Then you have your local set-up with MLflow running. As an advanced user, if you ant to collaborate with other people, you can have your own server running. However this is not what you need to start with at first. The python package MLflow contains four main modules:
- **Tracking:** focussed on experiemnt tracking
- **Models:** a special type of model
- **Model Registry**: useful to manage models in MLflow
- **Projects**: out of the scope for part of this course. But you are encouraged to look at the official MLFlow documentation. 

### MLflow `Experiment tracking`

When it comes to tracking experiment with MLflow, it is important to keep in mind that this module allows you to organize your experiments into **runs** and to keep track of the below for each run:

- ***Parameters***: also includes hyperparameter or any parameter that you think could have an effect on the metric of the model e.g. the path to the training dataset, because if you change the dataset, then path will be changed and it will be reflected in the experiment. Also if we apply some preprocessing to the data, we can include this as a parameter. 
- ***Metrics***: it includes any evaluation metrics that you can think of like Accuracy, F1 etc. We can include metric not only from the train dataset, but also from the test or validation dataset. 
- ***Metadata***: It includes any information that is related to the experiment, e.g. tags. This will allow to search and filter your runs easily. We can use a tag that is name of the developer or type of algorithm you are experimenting with. Then later if we want to compare runs, we caa filter easily using tags. 
- ***Artifacts***: It can be any file that says that you trained your model and you performed some visualization of the data. This will be considered an artifact. if you think it is important to keep this visualization for your run, then you can just log the artefact. You can also log the dataset as an artefact. But of course this doesn't scale well. This means that your data may be duplicated. Maybe you can think about a better solution for that.    
- ***Models***: You can log your model as part of the experiment. In certain cases it'll make sense to save the model. But if you are doing hyper parameter tuning, you don't care about the model. You just care about the performance of the model. What is the set of parameters that acheieved the best performance. 

Alongwith the above information MLflow automatically logs some extra information about the run:
- ***source code***: name of the file used to run in the experiment. 
- ***Version of the code (git commit)***: It basically saves the git commit.
- Start and end time
- Author

References:

* Link to the [video](https://www.youtube.com/watch?v=MiA7LQin9c8&list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&index=12)