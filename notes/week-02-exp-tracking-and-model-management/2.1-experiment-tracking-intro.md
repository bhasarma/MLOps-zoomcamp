# Experiment tracking

We'll see what experiment tracking is and see one tool called MLFlow. We'll see how to install the tool ad how to use it. 

Here we are not talking about A/B testing. 

Important concepts:
- ML experiment: the process of building an ML model- training, parameter tuning etc.
- Experiment run: each trial in an ML experiemnt
- Run artifact: any file that is associated with an ML run
- Experiment metadata: all the information that is released to the experiment. We'll see some examples later.

## What is Experiment tracking
It is the proces of keeping track of all the relevant information from an ML experiment, which includes: 
* Source code
* Environment
* Data
* Model
* Hyperparameters
* Metrics

## Why is Experiment tracking important

In general because of 3 main reasons:
* Reproducibility
* Organization
* Optimization

Most basic experiment tracking we can think of is e.g. writing in a piece of paper or in an excel sheet. Some probelems with these approches are:
- they are error prone
- No standard format
- Visibility and collaboration

## MLflow
Definition: An open source platform foe the machine learning life cycle.

In practice, it's just a python package unlike e.g. Kubeflow that can be installed with pip and it contains four main modules:
- Tracking: focussed on experiemnt tracking
- Models: a special type of model
- Model Registry: useful to manage models in MLflow
- Projects: out of the scope for part of this course

### MLflow `tracking`
This module allows you to organize your experiments into **runs** and to keep track of:

- ***Parameters***: also includes hyperparameter or any parameter that you think could have an effect on the metric of the model e.g. the path to the training dataset, because if you change the dataset, then path will be changed and it will be reflected in the experiment. Also if we apply some preprocessing to the data, we can include this as a parameter. 
- ***Metrics***: it includes any evaluation metrics that you can think of. Accuracy, F1 and so on. We can include metric not only from the train dataset, but also from the test or validation dataset. 
- ***Metadata***: It includes any information that is related to the experiment, e.g. tags. This will allow to search and filter your runs easily. We can use a tag that is name of the developer or type of algorithm you are experimenting with. Then later if we want to compare runs, we cna filter easily using tags. 
- ***Artifacts***: It can be any file that says that you trained your model and you performed some visualization of the data. This will be considered an artifact. if you think it is important to keep this visualization for your run, then you can just log the artefact. You can also log the dataset as an artefact. But of course this doesn't scale well. This means that your data may be duplicated. Maybe you can think about a better solution for that.    
- ***Models***: You can log your model as part of the experiment. In certain cases it'll make sense to save the model. But if you are doing hyper parameter tuning, you don't care about the model. You just care about the performance of the model. What is the set of parameters that acheieved the best performance. 

An ML run is each trial in ML experiment. One experiment is basically bunch of runs. For each of these runs, you can keep track of the above quantities. 

Alongwith this information MLflow automatically logs extra information about the run:
- ***source code***: name of the file used to run in the experiment. 
- ***Version of the code (git commit)***: It basically saved the git commit.
- Start and end time
- Author


References:

* Link to the [video](https://www.youtube.com/watch?v=MiA7LQin9c8&list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&index=12)